{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Representation of Unstructured Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Representation은 자연어(Natural Language)를 머신러닝 학습을 위한 숫자(Numeric Data)형태로 변환하는 형태를 의미하며, 사용하는 기법에 따라 머신러닝 학습에 있어 많은 차이를 보임.<br>\n",
    "- 대표적으로 BOW(Bag-Of-Words, One Hot Representation), Distrbuted Representation등의 기법이 존재함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"left\">[Representation 개념도]</h2> \n",
    "<p align=\"center\"><img src=\"img/text_ml_representation_001.jpg\" alt=\"Drawing\" style=\"width: 900px; height: 400px; align:center\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"left\">[Sequence & Feature 개념]</h2> \n",
    "<p align=\"center\"><img src=\"img/text_ml_representation_002.jpg\" alt=\"Drawing\" style=\"width: 700px; height: 400px; align:center\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"left\">[N-gram]</h2> \n",
    "n개의 연속된 token(word, char)등의 열로, 한개의 token보다 그 이상의 열이 정보를 표현하는데 있어 보다 풍부함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"img/text_ml_representation_004.jpg\" alt=\"Drawing\" style=\"width: 700px; height: 400px; align:center\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. BOW(Bag-Of-Words/One-Hot-Representation,국소표현) / Classifical Model Of NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정의) One-Hot이란 단 한개의 값만 1이고, 나머지는 0인 것을 의미<br>\n",
    "장점) Document의 Representation이 용이함.(직관적), Document에 등장하는 단어의 직관적인 해석이 가능<br>\n",
    "단점) 단어와 단어간 관게 표현이 곤란(강아지와 멍멍이는 비슷한 단어인가), 단어간 유사성(Similarity) 표현이 곤란. <br>&nbsp;&nbsp;&nbsp;&nbsp;이는 input features의 차원(dimension)을 크게 하는 현상이 발생할 수 있음(값이 대부분 0, too sparse).<br>&nbsp;&nbsp;&nbsp;&nbsp;데이터 부족으로 인한 차원의 저주(Curse of Dimensionality) 유발-> 모델링이 어려운 측면 존재<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;sparseness때문에 low accuracy초래<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;lack of semantic generalzation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"img/text_ml_representation_003.jpg\" alt=\"Drawing\" style=\"width: 800px; height: 250px; align:center\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Distributed Representation(분산표현, aka Word Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정의) 단어/문서를 d차원의 공간의 벡터로 표현<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 벡터 V(전체 단어 수)로 할때 벡터 D(단어를 표현하는 벡터)라고 할때 D는 V보다 훨씬 작다.<br>\n",
    "장점) distributional hypothesis(비슷한 의미(semantics)를 가진 단어는 비슷한 맥락(context)에 등장하는 경향이 있다/비슷한 분포의 단어나 언어 항목이 서로 유사한 의미를 가짐)에 따른 공간상의 위치로 표현하는 방법<br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(공간에 배치하여 비슷한 의미를 찾는다)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;단어 임베딩은 NLP에서 최첨단(state-of-the-art) 결과를 창출함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------\n",
    "\n",
    "<p align=\"center\"><img src=\"img/tip.jpeg\" alt=\"Drawing\" style=\"width: 50px; height: 30px; align:center\"/><h3 align=\"left\"> Embedding 이란 </h3> </p> \n",
    "\n",
    "- 임베딩(Embedding)은 X차원의 공간에 있는 데이터를 원하는 정보를 잘 저장하면서, Y라는 새로운 공간으로 보내는 f:X -> Y함수\n",
    "  예) 1만 차원의 문서의 유사도를 잘 보존하여 2차원 공간으로 보내는 것\n",
    "- 정보 보존의 대상이 무엇이냐에 따라 다양한 임베딩(Embedding) 방법이 존재\n",
    "- word2vec, doc2vec등은 \"어떤 정보를 보존\"하며 저차원 dense vector를 학습하는 것(sparse vector -> dense vector)\n",
    "\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"left\">[분산표현 개념도]</h2> \n",
    "<p align=\"center\"><img src=\"img/text_ml_representation_005.png\" alt=\"Drawing\" style=\"width: 800px; height: 250px; align:center\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"left\">[분산표현 example]</h2> \n",
    "<p align=\"center\"><img src=\"img/text_ml_representation_006.png\" alt=\"Drawing\" style=\"width: 700px; height: 300px; align:center\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 그림에서와 같이 각 차원(dimension)이 특정한 의미를 지니지는 않으며, 비슷한 단어/문서는 비슷한 벡터로 표현된다는 유사성(Similariy)에 관련된 문제임.<br>\n",
    "- 머신러닝 알고리즘 수행을 위해서는 텍스트 데이터를 인식하기 위한 벡터화 변형이 필요함. 벡터화 변환을 통해 공간상에 token들이 위치하게 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"left\">[Lookup Table을 통한 분산표현 example]</h2> \n",
    "<p align=\"center\"><img src=\"img/text_ml_representation_008.png\" alt=\"Drawing\" style=\"width: 700px; height: 400px; align:center\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 행렬(matrix)형태로 데이터를 가공하지 않은채, 단어나 문서를 벡터로 표현하는 방법(정해진 크기의 d 공간상의 벡터로 표현하는 방법이어서 Embedding을 한다라고 표현함)<br>\n",
    "- CBOW(Continus Bag of Words) / Skip-gram의 두가지 방식이 존재함.<br>\n",
    "- 단어 주변에 등장하는 다른 단어들의 분포(distribution)의 유사성(similarity)을 보존하는 벡터 표현 방법\n",
    "- 단어 동시 등장 정보(word’s of co-occurrence)를 보존"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래 그림에서와 같이 cat과 dog는 주위에 분포하는 단어 분포가 유사하여 두 단어(cat, dog)는 비슷한 벡터를 가짐. <br>\n",
    "  --> 'cat'이 들어갈 자리에 'dog'를 넣어도 큰 무리가 없음(비슷한 word embedding vector)<br>\n",
    "  --> 두 단어는 비슷한 문맥(contextual word distribution)을 가짐<br>\n",
    "<p align=\"center\"><img src=\"img/text_ml_representation_007.png\" alt=\"Drawing\" style=\"width: 700px; height: 400px; align:center\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"left\">[Word2Vec Summary]</h2> \n",
    "<p align=\"center\"><img src=\"img/text_ml_representation_009.png\" alt=\"Drawing\" style=\"width: 800px; height: 500px; align:center\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"left\">[Word2Vec 동작방식]</h2> \n",
    "- Word2Vec은 |V| class(Class개수) softmax regression을 이용하여 각 단어의 벡터를 학습시키는 classifer<br>\n",
    "- CBOW를 기준으로 봤을때 input vector는 Averaging을 통해 Input Vector를 구하고, Softmax Regression을 통해 해당 Context(구하고자 하는 위치에 있는)에 위치할수 있는 단어의 확률값을 구하게 된다.<br>\n",
    "- Window를 기준으로 이러한 과정을 반복하여 Lookup Table에서 각 단어의 feature value를 갱신하게 된다.\n",
    "<p align=\"center\"><img src=\"img/012_text_ml_cbow_conceptual_diagram.png\" alt=\"Drawing\" style=\"width: 700px; height: 400px; align:center\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 수식적으로 보면 average vector인 input vector에 대한 각 단어에 대한 내적(inner product)값을 구하여 해당 단어와의 거리를 구한다고 보면 된다.(가까운지 여부)\n",
    "<p align=\"center\"><img src=\"img/013_text_ml_word2vec_calculus.png\" alt=\"Drawing\" style=\"width: 600px; height: 200px; align:center\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- learning 과정을 통해 단어 벡터들이 input vector(contect vector)에 가깝게 이동하게 된다.<br>\n",
    "- 아래 그림에서 처럼 'cat','dog'는 비슷한 문맥(contextual word distribution)을 가지므로, 비슷한 word embedding을 가지게 됨.<br>\n",
    "- Context Vector(Input Vector) 와 차이가 많이 나는(Softmax Regression Loss) 'cat'은 학습량(벡터 변화량)도 크게 된다. --> 많이 움직이게 된다.\n",
    "- softmax regression을 통해 <b>알맞은 단어는 context vector방향으로 당기고, 적절하지 않은 단어는 반대 방향으로 밀어내는 효과</b>를 내게 된다.\n",
    "<p align=\"center\"><img src=\"img/014_text_ml_word2vec_learning_diagram.png\" alt=\"Drawing\" style=\"width: 800px; height: 250px; align:center\"/></p>\n",
    "<p align=\"center\"><img src=\"img/015_text_ml_word2vec_learning_diagram2.png\" alt=\"Drawing\" style=\"width: 450px; height: 250px; align:center\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"img/tip.jpeg\" alt=\"Drawing\" style=\"width: 50px; height: 30px; align:center\"/><h2 align=\"left\"> 내적(inner product) 이란 </h2> </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 두 벡터의 내적(inner product)란 한 벡터의 다른 벡터로의 projection을 의미하며, 두 벡터의 사이 각도가 0에 가까우면 1이 되며, 90에 가까우면 0이된다.(cosine값)<br>\n",
    "<img src=\"img/016_text_ml_inner_product_concept.png\" alt=\"Drawing\" style=\"width: 650px; height: 450px; align:center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"left\">[Word2Vec Visualization]</h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1020\"\n",
       "            height=\"1100\"\n",
       "            src=\"https://ronxin.github.io/wevi/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x120089b00>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://ronxin.github.io/wevi/', width=1020, height=1100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"left\">[CBOW vs Skip-Gram]</h2> \n",
    "- CBOW는 주위 단어로 현재 단어 w(t)를 예측하는 모델이며, skipgram은 현재 단어 w(t)로 주위 단어 모두를 예측하는 모델<br>\n",
    "\n",
    "- CBOW는 단어가 속한 context(target word주변에 고정 사이즈에 포함된 a bag of words)에 따라 target word를 예측<br>\n",
    "- input이 Context의 vector가 되며, output가 target word가 됨<br>\n",
    "- input vector가 average형태로 단어의 위치를 보지 않는 bag of words이므로, 학습 속도는 빠르나 skipgram에 비해서 성능은 떨어진다.\n",
    "- word order가 손실됨.\n",
    "\n",
    "- skip-gram은 target word가 input값이 되고(one hot code vector), output은 여러번 반복해서 선택된 수의 문맥 단어(context word)를 생성\n",
    "<p align=\"center\"><img src=\"img/011_text_ml_skip_gram_cbow_network.png\" alt=\"Drawing\" style=\"width: 700px; height: 500px; align:center\"/></p>\n",
    "- CBOW와 Skip-Gram으로 각각 선택하여 학습시 최종 결과의 Vector 그래프 형태는 아래와 같다.여러 논문에서 성능 비교를 진행했을 때, 전반적으로 <b><font color=\"blue\">Skip-gram이 CBOW보다 성능이 좋다</font></b>고 알려져 있습니다.\n",
    "<p align=\"center\"><img src=\"img/010_text_ml_skip_gram_cbow_graph.png\" alt=\"Drawing\" style=\"width: 1080px; height: 700px; align:center\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"left\">[Word2Vec Sample CODE]</h2> \n",
    "- 영화 리뷰 데이터 기반으로 Word2Vec 학습 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soynlp Install (Package)\n",
    "#!pip install soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soynlp\n",
    "\n",
    "corpus_fname = './data/comments_172movies/merged_comments.txt'\n",
    "tokenized_corpus_fname = './data/comments_172movies/merged_comments_tokenized_modify.txt'\n",
    "\n",
    "\n",
    "TRAIN_WORD2VEC = True\n",
    "word2vec_fname = './data/comments_172movies/movie_review_word2vec_model_v3.1.pkl'\n",
    "word2vec_fname_wv = './data/comments_172movies/movie_review_word2vec_model_wv_v3.1.kv'\n",
    "\n",
    "\n",
    "TRAIN_DOC2VEC = True\n",
    "doc2vec_fname = './data/comments_172movies/movie_review_doc2vec_model_v3.1.pkl'\n",
    "\n",
    "id2movie_fname = './data/comments_172movies/id2movie.pkl'\n",
    "id2actor_fname = './data/comments_172movies/id2actor.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized corpus file 5 row print\n",
    "with open(tokenized_corpus_fname, \"r\", encoding=\"utf-8\") as f:\n",
    "    i = 0\n",
    "    for i, doc in enumerate(f):\n",
    "        if i < 10:\n",
    "            print(doc)\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized Corpus File Parsing\n",
    "#[[영화아이디, 텍스트, 평점], \n",
    "# [영화아이디, 텍스트, 평점], \n",
    "# ...\n",
    "#]\n",
    "def get_text(fname, debug=True):\n",
    "    with open(fname, encoding='utf-8') as f:\n",
    "        docs = []\n",
    "        for i, doc in enumerate(f):\n",
    "            if debug and i >= 1000:\n",
    "                break\n",
    "            docs.append(doc.split('\\t')) #Tab으로 구분함. <영화 아이디, 텍스트, 평점>\n",
    "    \n",
    "    idx, texts, scores = zip(*docs) # Unpack 후 zipping\n",
    "    return idx, texts, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화 ID, 문서 내용(Text), 평점(Score) Get\n",
    "idx, docs, scores = get_text(tokenized_corpus_fname)\n",
    "docs[:5] #0~4까지 index를 가지는 문서 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "\n",
    "<p align=\"center\"><img src=\"img/tip.jpeg\" alt=\"Drawing\" style=\"width: 50px; height: 30px; align:center\"/><h4 align=\"left\"> Corpus File Processing Concept </h4> </p> \n",
    "<p align=\"center\"><img src=\"img/017_text_ml_corpus_processing_concept.png\" alt=\"Drawing\" style=\"width: 750px; height: 500px; align:center\"/></p>\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000개의 문서(리뷰)가 존재함을 확인 가능\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Package URL </b>\n",
    "- https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Word2Vec 학습</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class CommentWord2Vec:\n",
    "    \n",
    "    def __init__(self, fname):\n",
    "        self.fname = fname\n",
    "        if not os.path.exists(fname):\n",
    "            print('file not found: %s' % fname)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        with open(self.fname, encoding='utf-8') as f:\n",
    "            for doc in f:\n",
    "                movie_idx, text, score = doc.split('\\t')\n",
    "                yield text.split() # whitespace tokenization, 모든 리뷰들을 메모리에 올리는 것이 아니라 호출할때 마다 한번씩 호출되게 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_corpus = CommentWord2Vec(tokenized_corpus_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_doc, doc in enumerate(word2vec_corpus): # enumerate가 CommentWord2Vec의 __iter__를 호출하게 하고, loop를 돌면서 yield가 호출되게 된다.ㅠ\n",
    "    if num_doc > 10: break\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "default parameters를 이용하여 학습하며, Word2Vec의 arguments 중에서 중요한 것들은 아래와 같습니다. \n",
    "- size: 단어의 임베딩 공간의 크기\n",
    "- alpha: learning rate\n",
    "- window: 한 단어의 좌/우의 문맥 크기\n",
    "- min_count: 모델이 학습할 단어의 최소 출현 빈도수\n",
    "- max_vocab_size: None이 아닌 숫자를 입력하면 빈도수 기준으로 상위 max_vocab_size 개수만큼의 단어만 학습\n",
    "- sg: 1이면 skipgram 이용\n",
    "- negative: negative sampling에서 negative sample의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "\n",
    "#if TRAIN_WORD2VEC:\n",
    "if not True:\n",
    "    word2vec_model = Word2Vec(word2vec_corpus,\n",
    "                             size=100,\n",
    "                             alpha=0.025,\n",
    "                             window=5,\n",
    "                             min_count=5,\n",
    "                             sg=0,\n",
    "                             negative=5)\n",
    "    with open(word2vec_fname, 'wb') as f:\n",
    "        pickle.dump(word2vec_model, f)\n",
    "else:\n",
    "    with open(word2vec_fname, 'rb') as f:\n",
    "        word2vec_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습된 Word2Vec모델의 .most_similar(단어, topn) 함수는 입력된 단어에 대하여 가장 비슷한 topn개의 다른 단어들과 유사도를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim Word2Vec 확인 (주석 uncomment)\n",
    "#?gensim.models.Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 특정 단어의 vector값을 확인 할 수 있습니다. feature는 100차원입니다.(우리는 단어의 size(feature_dim)을 100으로 지정하여 학습하였습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv['영화']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv['한지민']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 로맨스와 가장 비슷한(similar)한 단어는 '멜로', '로멘스' 순으로 유사합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.most_similar('로맨스', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배우 '한지민'과 유사한 단어는 같은 배우인 '전지현','한가인','임수정'등으로 비슷함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.most_similar('한지민', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1점(최하위 점수)와 유사한 단어는 한굴로 쓴 '일점', 그 이후로는 1,2,3,... 이렇게 점수가 멀어져 감을 확인 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.most_similar('1점', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 9점(데이터셋 내 최상위 점수)와 유사한 단어는 한굴로 쓴 '8점', 그 이후로는 8,7,6,5,4,3,2,1,... 이렇게 점수가 멀어져 감을 확인 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.most_similar('9점', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- '십점'은 별 다섯개에 해당하여 '별5개'가 유사한 단어에 포함됨을 확인 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.most_similar('십점', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Olleh KT에서 '무비스타소셜클럽'을 진행하였던 평론가 '백은하'씨와 같은 단어로는 평론가 '황진미','이동진'등이 유사한 단어로 추출됨을 확인 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.most_similar('백은하', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- '주연', '조연'은 서로 밀접한 단어임을 확인 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.most_similar('주연', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.most_similar('조연', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- '메가박스'와는 동일한 영화관에 해당하는 단어들이 유사어로 확인됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.most_similar('메가박스', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'KBS'와 유사한 단어로는 같은 방속국이 높은 유사도로 상위에 랭크됨을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.most_similar('kbs', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습된 word2vec 모델의 shape는 다음과 같이 확인 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?word2vec_model.wv.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Word2Vec Save/Load</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train된 word2vec.wv를 저장할수 있습니다. (모델 자체가 아닌 KeyValue Vector구조를 저장합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = word2vec_model.wv\n",
    "word_vectors.save(word2vec_fname_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "word_vectors_wv = KeyedVectors.load(word2vec_fname_wv, mmap='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 상단의 결과와 동일하게 출력됨을 확인할수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_wv.most_similar('kbs', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load된 KeyValue Vector의 Shape는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_wv.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "<p align=\"center\"><img src=\"img/tip.jpeg\" alt=\"Drawing\" style=\"width: 50px; height: 30px; align:center\"/><h4 align=\"left\"> Normalization의 개념 </h4> </p> \n",
    "- normalization을 하는 이유는 over-fitting이나 under-fitting을 방지하기 위함, 최적화 과정에서 안정성/수렴속도 향상을 위한 것임.<br>\n",
    "- 모든 자료에 선형 변환을 적용 \n",
    "<p align=\"center\"><img src=\"img/018_text_ml_row_normalization_concept.png\" alt=\"Drawing\" style=\"width: 600px; height: 500px; align:center\"/></p>\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- feature가 100차원임을 확인 할수 있다. normalization된후 shape는 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_wv.vectors_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래 index에서 0을 70234(우리는 앞에서 word2vec 모델의 shape가 70235 X 100임을 확인하였습니다.)까지 임의의 숫자로 바꿔도 1근처의 값이 나오게 됩니다.<br>\n",
    "- cosine similarity를 위하여 row normalize한 행렬로 모양은 같지만, 벡터의 2 Norm이 1임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((v**2 for v in word2vec_model.wv.vectors_norm[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((v**2 for v in word_vectors_wv.vectors_norm[70234]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  단어별 인덱스(index2word)정보는 word2vec.wv.index2word(word frequency가 높은 것이 먼저 나타납니다) 저장되어 있습니다.\n",
    "-  앞서 확인한 word2vec의 shape (70235 X 100)과 동일한 단어개수가 70235임을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word2vec_model.wv.index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.index2word[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word2index는 index2word를 이용해 만들수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "for index, word in enumerate(word2vec_model.wv.index2word):\n",
    "    word2index[word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index['한지민']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어 관련된 정보는 word2vec_model.wv.vocab(KeyValue Vectors)을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.vocab['한지민']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- '한지민'이라는 단어는 2392번 사용되었다는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.vocab['한지민'].count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.vocab['손예진'].count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.vocab['정우성'].count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.vocab['송강호'].count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 영화 리뷰 이므로 '영화'라는 단어의 frequency가 가장 높고, 다음으로 '이', '관람객'이 많이 등장함을 알수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.wv.index2word[0], word2vec_model.wv.index2word[1], word2vec_model.wv.index2word[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(word2vec_model.wv.vocab['영화']))\n",
    "print(str(word2vec_model.wv.vocab['이']))\n",
    "print(str(word2vec_model.wv.vocab['관람객']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fasttext는 하나의 단어에 대하여 벡터를 직접 학습하지 않고, subwords의 벡터들을 바탕으로 word의 벡터를 추정합니다.(Word2Vec을 기본으로 하되 부분단어들을 임베딩하는 기법)<br>\n",
    "  subwords를 기준으로 학습을 하기 때문에 오탈자에 민감해지게 됨.\n",
    "- 띄어쓰기(whitespaces)를 기준으로 words를 구분하여 학습하는 형태임\n",
    "- 한국어는 초/중/종성에서 한군데 정도가 틀리기 때문에, 자음/모음을 풀어서 fasttext를 풀어서 학습하는게 좋음, 즉 '어디야'를 'ㅇㅓㄷㅣㅇㅑ'로 표현\n",
    "  종성이 비었을 경우 -를 삽입하는 형태가 적절함.\n",
    "- subwords는 반드시 2글자는 아니며, subwords의 frequency를 기준으로 fasttext모델이 적절한 수준의 subwords를 선택함.<br>\n",
    "  'ㅇㅓ-ㄷㅣ-ㅇㅑ-' = [ㅇㅓ-, ㄷㅣ, -ㅇㅑ-]\n",
    "- word2vec은 학습에 없는 단어를 찾을 경우 오류가 발생하나, fasttext는 training 데이터에 없어도 vector data를 추출할 수 있다.\n",
    "- 최근에는 fasttext를 embedding방법으로 사용하는 논문들이 많이 출간되는 상황임.\n",
    "- 단어 동시 등장 정보(word’s of co-occurrence)를 보존\n",
    "   \n",
    "<p align=\"center\"><img src=\"img/tip.jpeg\" alt=\"Drawing\" style=\"width: 50px; height: 30px; align:center\"/><h4 align=\"left\"> Normalization의 개념 </h4> </p> \n",
    "<p align=\"center\"><img src=\"img/019_text_ml_fasttext_learning_concept.png\" alt=\"Drawing\" style=\"width: 750px; height: 450px; align:center\"/></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.hangle import decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(s):\n",
    "    s_ = ' '\n",
    "    for c in s:\n",
    "        if c == ' ' and s_[-1] != ' ':\n",
    "            s_ += ' '\n",
    "            continue\n",
    "        jamo = decompose(c)\n",
    "        if (not jamo):\n",
    "            if s_[-1] != ' ':\n",
    "                s_ += ' '\n",
    "            continue\n",
    "        if (len(jamo) != 3) or (jamo[0] == ' ' or jamo[1] == ' '):\n",
    "            s_ += ' '\n",
    "            continue        \n",
    "        s_ += ''.join([jamo[0], jamo[1], '-' if jamo[2] == ' ' else jamo[2]])\n",
    "    return s_.strip() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessing('이것은 자모처리 라이브러리 내용입니다.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래는 전처리를 해둔 코드입니다. 실행은 하지 마시고, 각자의 데이터를 fasttext 학습용으로 만들 때, 코드를 수정해서 이용해주세요\n",
    "- subwords 를 이용한 단어 임베딩용 데이터와 supervised classifier 학습용 데이터를 동시에 만듭니다.\n",
    "- subword 를 이용한 단어 임베딩에서는 초/중/종성을 풀어서 임베딩 합니다.\n",
    "- supervised classifier 에서는 단어 그대로 이용합니다. classifier는 character n-grams 을 이용하고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_corpus_fname = './data/comments_10movies/merged_comments.txt'\n",
    "\n",
    "jamo_corpus_fname = './data/fasttext/merged_comments_jamo.txt'\n",
    "supervised_corpus_fname = './data/fasttext/merged_comments_supervised.txt'\n",
    "\n",
    "embedding_name = './data/fasttext/merged_comments_fasttext'\n",
    "supervised_name = './data/fasttext/merged_comments_classifier'\n",
    "\n",
    "if False:\n",
    "    with open(original_corpus_fname, encoding='utf-8') as fi:\n",
    "        with open(jamo_corpus_fname, 'w', encoding='utf-8') as f_uns:\n",
    "            with open(supervised_corpus_fname, 'w', encoding='utf-8') as f_sup:\n",
    "                for i_doc, doc in enumerate(fi):\n",
    "                    # for jamo_corpus\n",
    "                    sents = doc.split('\\t')[1].split()\n",
    "                    sents_ = [preprocessing(sent) for sent in sents]\n",
    "                    sents_ = '  '.join(sents_).strip()\n",
    "                    f_uns.write('{}\\n'.format(sents_))\n",
    "                    # for supervised\n",
    "                    rate = int(doc.split('\\t')[2].strip())\n",
    "                    if rate >= 8:\n",
    "                        rate = 'positive'\n",
    "                    elif rate <= 3:\n",
    "                        rate = 'negative'\n",
    "                    else:\n",
    "                        continue\n",
    "                    f_sup.write('__label__{} {}\\n'.format(rate, doc.split('\\t')[1]))\n",
    "                    \n",
    "                    if i_doc % 100 == 99:\n",
    "                        print('\\r  - preprocessing ... {} docs'.format(i_doc+1), flush=True, end='')\n",
    "    print('\\ndone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install Cython\n",
    "#!pip install fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 오리지널 데이터 확인 (영화 ID/영화 평/영화 평점) Corpus File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(original_corpus_fname, encoding='utf-8') as f:\n",
    "    for _ in range(5):\n",
    "        print(next(f).strip()[:50],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 초성/중성/종성 자모 프로세싱 Corpus File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(jamo_corpus_fname, encoding='utf-8') as f:\n",
    "    for _ in range(5):\n",
    "        print(next(f).strip()[:50],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentiment Analysis(Positive/Negative) Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(supervised_corpus_fname, encoding='utf-8') as f:\n",
    "    for _ in range(5):\n",
    "        print(next(f).strip()[:50],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><h4 align=\"left\"> Word representation using subword embedding </h4> </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 자모처리(초성/중성/종성)된 Corpus File을 가지고 Fasttext를 Train(Word representation learning)\n",
    "- Train을 하게 되면 bin 파일과 vec 파일을 저장하게 됩니다.<br>\n",
    "  <font color='blue'>model.vec is a text file containing the word vectors, one per line.<br></font>\n",
    "  <font color='blue'>model.bin is a binary file containing the parameters of the model along with the dictionary and all hyper parameters.<br>The binary file can be used later to compute word vectors or to restart the optimization.</font>\n",
    "- Train을 하시면 bin파일과 vec파일이 저장됨을 확인 할 수 있다.\n",
    "- word2vec과 마찬가지로 skipgram 모델의 성능이 높은 것으로 알려져 있습니다.\n",
    "- cbow는 surrounding windows내의 모든 words의 vector값의 sum을 사용하고, 이를 target word를 예측하는데 사용한다.<br>\n",
    "- skipgram model은 random close-by word를 사용해 target word를 예측합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"img/021_text_ml_fasttext_cbow_skipgram.png\" alt=\"Drawing\" style=\"width: 700px; height: 400px; align:center\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 상세 parameter 설정은 다음의 url(https://pypi.org/project/fasttext/) 에서 확인하시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fasttext is a Python interface for Facebook fastText.\n",
    "import fasttext\n",
    "\n",
    "TRAIN_FASTTEXT = False\n",
    "\n",
    "if TRAIN_FASTTEXT:\n",
    "    #embedding_model = fasttext.cbow(jamo_corpus_fname, embedding_name, minn=3, maxn=6, thread=8)\n",
    "    #Corpus File Name, Embedding File Name, min length of char ngram, max length of char ngram,number of threads \n",
    "    embedding_model = fasttext.skipgram(jamo_corpus_fname, embedding_name, minn=3, maxn=6, thread=8) \n",
    "else:\n",
    "    embedding_model = fasttext.load_model(embedding_name+'.bin', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- '영화관' 단어의 word vector는 다음과 같이 확인 할 수 있습니다.(100차원의 word vector를 확인할 수 있습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model['영화관']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model['Circle']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 초성/중성/종성을 나눴기 때문에 cosine similarity를 계산할때도 입력될 단어를 초성/중성/종성으로 나눠서 해야 cosine similarity를 계산할수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(word1, word2):\n",
    "    word1 = preprocessing(word1) # 초성/중성/종성 decompose\n",
    "    word2 = preprocessing(word2) # 초성/중성/종성 decompose\n",
    "    return embedding_model.cosine_similarity(word1, word2)\n",
    "\n",
    "cosine_similarity('영화관', '메가박스')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><h4 align=\"left\"> Gensim Representation </h4> </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "hai_corpus_fname = './data/hai/classification_train_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(hai_corpus_fname, encoding='utf-8') as f:\n",
    "    for _ in range(5):\n",
    "        print(next(f).strip()[:50],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(hai_corpus_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pandas에서는 dataframe의 summary정보를 확인할 수 있도록 describe 메소드를 제공합니다.\n",
    "- utterance(9142개), intent(11개)의 데이터 셋입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- intent별 속한 utterance의 개수(데이터 개수)를 다음과 같이 확인 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['intent_nm'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dataframe의 상위 5개 row를 출력해 확인 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "#size = dimension size\n",
    "config = {'size' : 32, 'min_count' : 1, 'word_ngrams' : 1, 'min_n' : 1,\n",
    "          'alpha' : 0.025, 'min_alpha' : 0.025, 'iter' : 500, 'window' : 5}\n",
    "fasttext_model = FastText(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = data['utterance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances = data[\"utterance\"].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary\n",
    "fasttext_model.build_vocab(sentences=utterances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train document vectors\n",
    "fasttext_model.train(sentences=utterances, total_examples=fasttext_model.corpus_count, epochs=fasttext_model.epochs)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_train = [fasttext_model[x] for x in utterances]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fasttext로 전체 utterance에 대해서 train한후 train vector값을 확인합니다. 첫 utterance는 '[Good, evening]'이므로 2개의 vector값을 가집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fasttext_train = [np.mean(x, axis=0) for x in fasttext_train]\n",
    "fasttext_train[0] # 실제 1st row의 vector값이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stack the vectors\n",
    "fasttext_train = np.stack(fasttext_train, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 현재 까지의 과정을 도식화 하면 아래와 같은 그림이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"img/022_text_ml_fasttext_mean_vector.png\" alt=\"Drawing\" style=\"width: 600px; height: 400px; align:center\"/></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[FastText] train data shape: {}\".format(fasttext_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train된 모델을 기반으로 단어에 대한 vectorize된 결과를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model.wv['연차']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model.wv['Circle']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gensim을 통해서도 fasttext 모델을 저장할수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "fasttext_model_fname = './data/fasttext/merged_comments_fasttext.model'\n",
    "#fname = get_tmpfile(fasttext_model_fname)\n",
    "fasttext_model.save(fasttext_model_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 저장된 fasttext model을 로드하여 확인해보면 train직후 모델의 값과 동일함을 확인 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_reload_model = FastText.load(fasttext_model_fname)\n",
    "fasttext_reload_model.wv['Circle']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-3. Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 미국 스탠포드대 NLP랩에서 개발함\n",
    "- Glove Python Package를 통해서 Glove를 사용가능합니다.\n",
    "- 'pip install glove'가 오류가 나므로, 아래 명령을 통해서 설치를 합니다.(https://github.com/JonathanRaiman/glove/issues/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install https://github.com/JonathanRaiman/glove/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = './data/corpus/2016-10-28_article_all_normed.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파일을 읽기 위해 with를 이용해서 open하여 corpus의 내용을 읽어들입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(corpus_file, 'r') as f:\n",
    "   sents = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 실행시간이 너무 길어지는 것을 예방하기 위해 상위 5000개의 라인만을 확인 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = sents[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 상위 3개 문서 내용을 확인 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \\n',\n",
       " '위키리크스 클린턴 전 대통령 최측근 메모 공개  워싱턴 연합뉴스 신지홍 특파원 미국 민주당 대선후보 힐러리 클린턴의 남편인 빌 클린턴 전 대통령이 자신이 고문으로 속한 한 기업을 통해 고액강연을 주선 받거나 가족재단인 클린턴재단 에 수천만 달러의 기부금이 흘러들어가도록 한 것으로 밝혀졌다고 미 언론이 27일 현지시간 전했다  클린턴 전 대통령이 그 대가로 어떤 도움을 줬는지는 알 수 없지만 공화당 대선후보 도널드 트럼프가 주장해온 클린턴재단 과 힐러리가 장관을 지낸 국무부 간의 유착의혹 등을 상기시키는 정황이다  위키리크스가 공개한 클린턴 전 대통령의 측근 더글러스 밴드가 과거 기록한 12쪽짜리 메모에 따르면 클린턴 전 대통령은 힐러리가 국무장관으로 재직하던 2011년 당시 밴드의 컨설팅회사인 테네오 의 고문으로 활동했다  클린턴재단의 기금모금자로 10년 이상 활동해온 밴드는 당시 코카콜라와 다우케미칼 등 대기업 임원들과의 친분을 바탕으로 클린턴재단에 수백만 달러의 기부금을 내도록 했다  그런가 하면 대형은행인 도 클린턴재단에 54만 달러를 기부토록 했다 추후 는 클린턴 전 대통령에게 총 3차례에 걸쳐 강연 기회를 주고 90만 달러를 지급했다  2011년 11월자 메모에 따르면 이렇게 클린턴 전 대통령에게 보장해준 유급강연 등 비즈니스 주선 은 3천만 6천만 달러 343억 686억 원 에 달한다  밴드는 메모에서 자신의 이러한 일을 병행적 이지만 서로 독립적이라고 밝히면서 이러한 독특한 역할을 통해 우리는 클린턴 전 대통령의 개인적 정치적 사업적 목표와 클린턴재단의 비영리 목표를 동시에 수행하는 등 균형을 맞출 수 있다 고 주장했다  하지만 의회전문매체인 더 힐 은 이 메모는 클린턴재단과 클린턴 가족의 비즈니스 간 뒤얽힘을 보여준다 며 밴드는 비영리기구를 위한 기부금을 걷으면서 동시에 전직 대통령을 위한 영리 기회를 보장했다 고 지적했다  이에 대해 테네오 측은 클린턴재단이 전 세계에서 하는 좋은 일을 지원하기 위해 기부금을 기업들에 요청한 것 이라며 우리 회사는 이 일과 관련해 어떤 금전적 혜택도 받지 않았다 고 말했다  2016년 10월19일 라스베이거에서 유세하는 대선후보 힐러리 클린턴과 남편 빌 연합뉴스 자료사진\\n',\n",
       " '워싱턴 연합뉴스 심인성 특파원 미국 공화당 대선후보 도널드 트럼프가 26일 현지시간 민주당 지도부와 민주당 대선후보 힐러리 클린턴 캠프 내부 이메일해킹 사건의 배후와 관련해 북한이나 중국일 가능성도 있다고 주장했다  해킹 사건의 배후로 자신과 러시아의 연계설을 제기하는 민주당의 주장을 방어하는 과정에서 뜬금없이 두 나라를 끌어들인 것이다  26일 노스캐롤라이나 주 샬럿 유세장의 도널드 트럼프 연합뉴스 자료사진  트럼프는 이날 노스캐롤라이나 주 킨스턴 유세에서 폭로전문사이트 위키리크스가 뭔가 폭로하기만 하면 민주당은 트럼프와 러시아가 공모한 것 이라고 하는데 이는 말도 안 되는 얘기 라고 말했다  이어 그들은 솔직히 러시아 소행인지도 잘 모른다 면서 누가 알겠느냐 배후가 러시아일 수도 있고 중국일 수도 있다 소니 해킹 사건을 기억하는지 모르겠는데 그 사건의 배후인 북한일 수도 있다 고 주장했다  그러면서 다른 많은 곳 나라 들이 배후 일 수 있다 고 덧붙였다  호주 출신 줄리언 어산지 44 가 설립한 위키리크스는 지난 7월 말 경선 불공정 관리 의혹이 담긴 민주당 전국위원회 지도부 인사 7명의 이메일을 대거 폭로해 거센 논란을 야기한 데 이어 최근에는 존 포데스타 클린턴캠프 선대본부장 등의 이메일을 잇따라 공개하고 있다  민주당은 현재 러시아가 트럼프의 당선을 돕기 위해 이메일을 해킹하고 위키리크스가 이를 폭로하는 것이라고 비판하고 있다  김정은 북한 노동당 위원장과 중국 국기 오성홍기 연합뉴스 제공\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 공기행렬 구축\n",
    "GloVe의 입력 데이터는 다음과 같은 dict 구조의 공기행렬(Cooccurrence Matrix)입니다.\n",
    "<p align=\"center\"><img src=\"img/034_text_ml_glove_cooccurrence_matrix.png\" alt=\"Drawing\" style=\"width: 600px; height: 300px; align:center\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CounterVectorizer는 링크(https://goo.gl/QPH2wT) 를 참조하시면 됩니다.\n",
    "- CounterVectorizer는 corpus파일을 입력받아 token count를 측정하는데 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# co-occurrence matrix 만들기\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=10, ngram_range=(1,1)) # min_df : document에서 10번 이상 나오는 단어, uni-gram사용(한개 단어만 봄)\n",
    "# Corpus를 가지고 CounterVectorizer를 학습합니다.\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 7057)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X의 shape를 보면 5000개의 document, 7057개의 dimension(word or token)으로 구성됨을 확인 할 수 있습니다.\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7057)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[4999].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X가 5000 X 7057 행렬 이므로, X.T는 7057 X 5000, X는 5000 X 7057이므로 이 둘의 matrix multiplication은 7057 X 7057이 됩니다.\n",
    "<p align=\"center\"><img src=\"img/035_text_ml_cooccurrence_matrix.png\" alt=\"Drawing\" style=\"width: 500px; height: 300px; align:center\"/></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xc = X.T * X             # CounterVectorizer를 기반으로 정사각 행렬(co-occurrence matrix)로 만든다.\n",
    "Xc.setdiag(0)\t\t\t # 대각성분을 0으로\n",
    "result = Xc.toarray()    # array로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7057, 7057)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X.T * X를 통해 정사각행렬이 됩니다.\n",
    "Xc.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7057"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dimension(word or token)에 해당하는 길이를 확인할 수 있습니다.\n",
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "for idx1, word1 in enumerate(result): # 행 (row)\n",
    "\ttmpdic = {}\n",
    "\tfor idx2, word2 in enumerate(word1): # 열 (column)\n",
    "\t\tif word2 > 0:\n",
    "\t\t\ttmpdic[idx2] = word2 # co-occurrence가 0인 대상에 대해서 값을 저장한다.\n",
    "\tdic[idx1] = tmpdic # 몇번째 단어는 어떤 단어들과 co-occurrence한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7057\n",
      "630\n",
      "5829\n"
     ]
    }
   ],
   "source": [
    "print(len(dic)) # 7057개의 단어(word)\n",
    "print(len(dic[0])) # 1번째 단어는 총 630개의 단어와 co-occurrence\n",
    "print(len(dic[2])) # 2번째 단어는 총 5829개의 단어와 co-occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CounterVectorizer는 Vocabulary를 가집니다. frequency가 상위 10개를 확인하면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('클린턴', 6345),\n",
       " ('대통령', 1833),\n",
       " ('최측근', 6191),\n",
       " ('공개', 890),\n",
       " ('워싱턴', 4457),\n",
       " ('연합뉴스', 4217),\n",
       " ('특파원', 6438),\n",
       " ('미국', 2461),\n",
       " ('민주당', 2507),\n",
       " ('대선후보', 1811)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vectorizer.vocabulary_.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dic은 단어 ID들로만 구성된 dict이므로 단어 리스트(id로 단어를 추출할수 있도록)들도 별도로 빼서 가지고 있는게 좋을 겁니다. 그 코드는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 리스트 작성\n",
    "import operator\n",
    "vocab = sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1)) #frequency를 기준으로 정렬을 합니다.(오름차순)\n",
    "vocab = [word[0] for word in vocab] # 단어들을 순서대로 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10개', '10년', '10년간', '10대', '10만', '10명', '10시', '10여', '10월', '10월까지']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#20번째 부터 30번째 까지 10개 단어를 확인 합니다.\n",
    "vocab[20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Glove의 Train Parameter는 아래와 같습니다. 상세 내역은 링크를 확인하시기 바랍니다. <br> https://pypi.org/project/glove/\n",
    "\n",
    "<b>Glove.init()</b><br>\n",
    "cooccurence dict<int, dict<int, float» : the co-occurence matrix<br>\n",
    "alpha float : (default 0.75) hyperparameter for controlling the exponent for normalized co-occurence counts.<br>\n",
    "x_max float : (default 100.0) hyperparameter for controlling smoothing for common items in co-occurence matrix.<br>\n",
    "d int : (default 50) how many embedding dimensions for learnt vectors<br>\n",
    "seed int : (default 1234) the random seed<br><br>\n",
    "    \n",
    "<b>Glove.train</b><br>\n",
    "step_size float : the learning rate for the model<br>\n",
    "workers int : number of worker threads used for training<br>\n",
    "batch_size int : how many examples should each thread receive (controls the size of the job queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, error 0.029\n",
      "epoch 1, error 0.022\n",
      "epoch 2, error 0.018\n",
      "epoch 3, error 0.016\n",
      "epoch 4, error 0.014\n",
      "epoch 5, error 0.013\n",
      "epoch 6, error 0.012\n",
      "epoch 7, error 0.011\n",
      "epoch 8, error 0.010\n",
      "epoch 9, error 0.010\n",
      "epoch 10, error 0.009\n",
      "epoch 11, error 0.009\n",
      "epoch 12, error 0.008\n",
      "epoch 13, error 0.008\n",
      "epoch 14, error 0.008\n",
      "epoch 15, error 0.007\n",
      "epoch 16, error 0.007\n",
      "epoch 17, error 0.007\n",
      "epoch 18, error 0.007\n",
      "epoch 19, error 0.007\n",
      "epoch 20, error 0.007\n",
      "epoch 21, error 0.007\n",
      "epoch 22, error 0.007\n",
      "epoch 23, error 0.007\n",
      "epoch 24, error 0.007\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "import glove\n",
    "model = glove.Glove(dic, d=100, alpha=0.75, x_max=100.0) # d : dimesion, alpha : nomalize, x_max : smoothing controlling\n",
    "for epoch in range(25):\n",
    "    # batch_size : how many examples should each thread receive (controls the size of the job queue)\n",
    "    # workers : number of worker threads used for training\n",
    "    err = model.train(batch_size=200, workers=4) \n",
    "    print(\"epoch %d, error %.3f\" % (epoch, err), flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습된 Glove의 벡터 추출과 피클 저장 관련 코드는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word vector 추출\n",
    "wordvectors = model.W\n",
    "\n",
    "# word vector 저장\n",
    "import pickle\n",
    "with open('./data/glove/glove', 'wb') as f: # write binary\n",
    "\tpickle.dump([vocab,wordvectors],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09206931,  0.01272863,  0.01322941,  0.05959304, -0.0391509 ,\n",
       "       -0.09398961, -0.01243014,  0.09309144,  0.03683872, -0.06559277,\n",
       "       -0.00768358, -0.27424329,  0.17491328,  0.09855931, -0.01252397,\n",
       "        0.13607569,  0.02742089,  0.27985828,  0.01588244, -0.01849626,\n",
       "       -0.11579127, -0.08869851, -0.06545905, -0.02426515,  0.03435246,\n",
       "       -0.12222193,  0.07944449,  0.30592845,  0.06107514, -0.1685199 ,\n",
       "       -0.0673388 ,  0.06180723, -0.01589304,  0.15157896,  0.04074443,\n",
       "       -0.06165527,  0.05466063,  0.12892201, -0.18967423,  0.05846294,\n",
       "        0.02103585,  0.09352659,  0.17083927, -0.1883815 , -0.04872999,\n",
       "       -0.19791622,  0.29458625, -0.13195139,  0.01763593,  0.06322306,\n",
       "       -0.03724963,  0.00879426, -0.03893306, -0.06909591, -0.00720039,\n",
       "       -0.08260639, -0.08804982,  0.04746568, -0.02528355,  0.13655067,\n",
       "        0.07914064,  0.08954631,  0.10347784,  0.07500438, -0.03075784,\n",
       "       -0.18628092, -0.10012397, -0.18901665,  0.01350933,  0.01915474,\n",
       "        0.13382779, -0.14070093,  0.04278539,  0.18479124,  0.15284555,\n",
       "       -0.06169986, -0.02776484, -0.03105207,  0.06515531,  0.01938302,\n",
       "        0.09842043, -0.09750866, -0.07140071,  0.10602262,  0.06983411,\n",
       "        0.09318149, -0.15243936, -0.11760058, -0.11643249, -0.05052023,\n",
       "        0.06982982, -0.01615836,  0.0484931 , -0.09723738,  0.02323663,\n",
       "       -0.02389808,  0.06044018, -0.03619363,  0.08012227,  0.19648802])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 20번째 단어('10개')의 word vector를 확인해 봅니다.\n",
    "wordvectors[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 우리는 앞에서 glove를 train할때 d=100으로 지정하였습니다. 따라서, vector 차원이 100차원이 됩니다.\n",
    "len(wordvectors[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- glove는 word2vec처럼 most_similar 함수가 없으므로 직접 만들어서 유사어를 판별하도록 합니다.\n",
    "- scipy의 cosine distance(https://goo.gl/VHPBvy)를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity computation method\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def most_similar(word, vocab, vecs, topn=10):\n",
    "\tquery = vecs[vocab.index(word)] # 특정 단어의 index값을 구하고, wordvector를 통해서 vector값을 구합니다.\n",
    "\tresult = []\n",
    "\tfor idx, vec in enumerate(vecs): # 전체 wordvectors를 loop를 돌면서\n",
    "\t\tif idx is not vocab.index(word): # loop상의 단어와 찾고자 하는 단어가 index가 다르면\n",
    "\t\t\tresult.append((vocab[idx],1-cosine(query,vec))) # result에 그 거리값(cosine similarity = 1 - cosine(query,vec))을 append합니다.\n",
    "\tresult = sorted(result,key=lambda x: x[1],reverse=True) # 상위 topn개를 추출하기 위해 cosine similarity 기준으로 역순으로 정렬합니다.\n",
    "\treturn result[:topn] # 상위 topn개 만큼 리턴합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- word('영화')라는 단어와 유사한 단어를 상위 5개를 추출합니다.\n",
    "- '영화' 라는 단어와 '혼자','맡아','배우','여자'와 유사하다고 나옵니다. 조금 거리가 있어 보이는 단어가 나온 것은 데이터가 적어서 이며, corpus자체를 늘려주면 보다 유사한 단어가 나타나게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('영화', 1.0),\n",
       " ('혼자', 0.5383186462428523),\n",
       " ('맡아', 0.5183374428261484),\n",
       " ('배우', 0.5014274051871304),\n",
       " ('여자', 0.4939673684003728)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar(word='영화', vocab=vocab, \n",
    "             vecs=wordvectors, topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 결과를 코드를 하나 하나 분해하면서 이해해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4261"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word('영화')는 4261번째 word입니다.\n",
    "vocab.index('영화')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.11757419, -0.15394233, -0.15851822, -0.16444426, -0.08197222,\n",
       "       -0.07201471, -0.5428812 , -0.02353871,  0.21546092, -0.12788594,\n",
       "        0.06516221,  0.00286803, -0.36160477,  0.27663266, -0.36529849,\n",
       "        0.06508999,  0.37628137,  0.40554597, -0.26624659,  0.04930825,\n",
       "        0.13820588,  0.11336907, -0.19879932, -0.16854739,  0.19082494,\n",
       "        0.31967215, -0.30940592,  0.05138069, -0.53670034,  0.05120749,\n",
       "        0.21276578,  0.01883632,  0.17206311,  0.08835532,  0.11650229,\n",
       "        0.07476148, -0.29170707, -0.46771875,  0.58073737,  0.02043927,\n",
       "       -0.10359153,  0.58316492,  0.07833701, -0.02281482, -0.090782  ,\n",
       "       -0.17104167, -0.20704959,  0.20886657, -0.26568241, -0.13464774,\n",
       "       -0.06324836,  0.3425581 , -0.28960971, -0.24767271,  0.59824767,\n",
       "        0.21276844, -0.33667971, -0.15480283,  0.1247204 , -0.17627071,\n",
       "       -0.24905625,  0.20297362,  0.35080402, -0.18170931,  0.09236859,\n",
       "       -0.14463487, -0.07822699,  0.58588433,  0.14851971, -0.07646184,\n",
       "        0.42104277, -0.14948457, -0.01859545,  0.15075936, -0.15135785,\n",
       "       -0.00251041, -0.47496592, -0.23153742, -0.09312737,  0.45986687,\n",
       "       -0.70509709, -0.08125114, -0.18265088, -0.0131386 , -0.2965524 ,\n",
       "        0.1030793 , -0.28860829, -0.04914759,  0.3927818 , -0.4890796 ,\n",
       "       -0.26969375, -0.03100368, -0.01292424, -0.13496262,  0.09964267,\n",
       "       -0.47191465,  0.16095408,  0.14091902,  0.12919765,  0.15276313])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word('영화')의 wordvector는 아래와 같습니다. 4261번째 단어의 wordvector값에 해당합니다.\n",
    "wordvectors[vocab.index('영화')] # query(내가 찾고자하는 단어의 벡터값은 query로 표시합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7057, 100)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 우리는 위에서 7057개의 문서를 대상으로, dimension=100 으로 train하였습니다.\n",
    "wordvectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 처리 과정을 도식화 하여 다시 표시하면 다음과 같습니다. \n",
    "- index가 일치하지 않는 wordvector와 query의 cosine similarity를 통해서 거리를 구하고 이를 저장합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\"><img src=\"img/036_text_ml_glove_most_similar.png\" alt=\"Drawing\" style=\"width: 650px; height: 500px; align:center\"/></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
